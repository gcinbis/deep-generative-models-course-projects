{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ceng796-gorkem-emre-dualgan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mh1H5tvuG-ln",
        "colab": {}
      },
      "source": [
        "__author__ = \"Emre Kulah, Arif Gorkem Ozer\"\n",
        "__email__ = \"emre.klh@gmail.com, arifgorkemozer@gmail.com\"\n",
        "\n",
        "__copyright__ = \"\"\"\n",
        "Copyright 2020, \n",
        "Middle East Technical University CENG-796  DEEP GENERATIVE MODELS\n",
        "\"\"\"\n",
        "__license__ = \"MIT\"\n",
        "__version__ = \"1.0.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z40D99O4DCRS",
        "colab_type": "text"
      },
      "source": [
        "This Jupyter notebook contains an implementation of the paper [DualGAN: Unsupervised Dual Learning for Image-to-Image Translation](https://arxiv.org/abs/1704.02510).\n",
        "\n",
        "**Authors:** Zili Yi, Hao (Richard) Zhang, Ping Tan, Minglun Gong\n",
        "\n",
        "**Published at:** International Conference on Computer Vision, 2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edjuweJA9hS3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Abstract\n",
        "Conditional Generative Adversarial Networks (GANs)\n",
        "for cross-domain image-to-image translation have made\n",
        "much progress recently. Depending\n",
        "on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However,\n",
        "human labeling is expensive, even impractical, and large\n",
        "quantities of data may not always be available. Inspired\n",
        "by dual learning from natural language translation,\n",
        "we develop a novel dual-GAN mechanism, which enables\n",
        "image translators to be trained from two sets of unlabeled\n",
        "images from two domains. In our architecture, the primal\n",
        "GAN learns to translate images from domain U to those in\n",
        "domain V , while the dual GAN learns to invert the task.\n",
        "The closed loop made by the primal and dual tasks allows\n",
        "images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with\n",
        "unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can\n",
        "even achieve comparable or slightly better results than conditional GAN trained on fully labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMMMDUvD9eS4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### How to run?\n",
        "\n",
        "*    By default, this notebook is configured to run on the **test mode**. Test mode allows you to load pre-trained models and get the results according to their implicit values. To run in **training mode**, you should change **\"test\"** parameter of **\"opt\"** to **False**.\n",
        "\n",
        "*    In test mode, pre-trained models are downloaded from a Google Drive directory. Once the pre-trained models are downloaded, they are ready to use. Downloading them once is enough.\n",
        "\n",
        "*    Hyper-parameters can be modified by changing values of **\"opt\"** variable in **Configuration** section in this notebook. \n",
        "\n",
        "*    Available datasets in this notebook are **\"facades\" and \"maps\"**. \"facades\" dataset is about generating sketches from pictures of facades and vice versa. \"maps\" dataset is for generating sketches from satelite views and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDUdI2-CBzW4",
        "colab_type": "text"
      },
      "source": [
        "# Importing Modules\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3BbRmlxLin7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import argparse\n",
        "\n",
        "import itertools\n",
        "import sys\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch.autograd as autograd\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import vgg19\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from torch.nn import init, modules\n",
        "from torchvision.utils import save_image\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI9-HXKRVvDN",
        "colab_type": "text"
      },
      "source": [
        "Cuda setting (in training):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFPMRxXcVs0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNvyFwfGGQBb",
        "colab_type": "text"
      },
      "source": [
        "# Classes For Generator and Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX1qQax8Gf75",
        "colab_type": "text"
      },
      "source": [
        "The main classes used in this implementation are:\n",
        "\n",
        "1.   UNetModule\n",
        "2.   Generator\n",
        "3.   DiscModule\n",
        "4.   Discriminator\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05sSex3LGzWe",
        "colab_type": "text"
      },
      "source": [
        "### UNetModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-9UObrWG4sz",
        "colab_type": "text"
      },
      "source": [
        "This class is used to represent partitions of the UNet model described [here.](https://arxiv.org/pdf/1505.04597.pdf) Each UNetModule can contain convolutional network, batch normalization block, activation block, dropout block. Since the **Generator** class has a UNet structure, it has members in type of UNetModule class. \n",
        "\n",
        "If UNetModule instance belongs to UNet segments going down, following operations are done while forwarding data to UNetModules below:\n",
        "\n",
        "1.   Activation block is applied.\n",
        "2.   Convolutional block is applied.\n",
        "3.   Batch normalization is applied.\n",
        "4.   Dropout is applied.\n",
        "\n",
        "If UNetModule instance belongs to UNet segments going up, following operations are done while forwarding data to UNetModules above:\n",
        "\n",
        "1.   Activation block is applied.\n",
        "2.   Convolutional block is applied.\n",
        "3.   Batch normalization is applied.\n",
        "4.   Dropout is applied.\n",
        "5.   Tanh() is applied (only in output layer, at end of the UNet).\n",
        "\n",
        "Not every segment of UNet has the exact same configurations, some of the operations mentioned above may not be applied to a particular UNetModule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5bOZh5TGWfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNetModule(nn.Module):\n",
        "    def __init__(self, down=True, input_size=3, output_size=3, kernel_size=4,\n",
        "                 stride=2, padding=1, activation=True, batch_norm=True, dropout=-1.0, output_layer=False, name=\"\"):\n",
        "        super(UNetModule, self).__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.down = down\n",
        "        self.conv_block = nn.Conv2d(in_channels=input_size, out_channels=output_size,\n",
        "                                    kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.act_block = nn.LeakyReLU(0.2, True)\n",
        "        self.bn_block = nn.BatchNorm2d(output_size)\n",
        "        self.do_block = None\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "        # conv transpose and relu as default activation for up\n",
        "        if not down:\n",
        "            self.conv_block = torch.nn.ConvTranspose2d(in_channels=input_size, out_channels=output_size,\n",
        "                                                       kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "            self.act_block = torch.nn.ReLU(True)\n",
        "\n",
        "        if not activation:\n",
        "            self.act_block = None\n",
        "        if not batch_norm:\n",
        "            self.bn_block = None\n",
        "        if dropout != -1.0:\n",
        "            self.do_block = nn.Dropout(dropout)\n",
        "\n",
        "    def down_forward(self, x):\n",
        "        # apply batch norm if exists\n",
        "        if self.bn_block:\n",
        "            # apply activation if exists\n",
        "            if self.act_block:\n",
        "                out = self.bn_block(self.conv_block(self.act_block(x)))\n",
        "            else:\n",
        "                out = self.bn_block(self.conv_block(x))\n",
        "        else:\n",
        "            # apply activation if exists\n",
        "            if self.act_block:\n",
        "                out = self.conv_block(self.act_block(x))\n",
        "            else:\n",
        "                out = self.conv_block(x)\n",
        "\n",
        "        # apply dropout if exists\n",
        "        if self.do_block:\n",
        "            out = self.do_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def up_forward(self, x, skip):\n",
        "        out = self.down_forward(x)\n",
        "        if self.output_layer:\n",
        "            out = torch.nn.Tanh()(out)\n",
        "        else:\n",
        "            out = torch.cat([out, skip], 1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        if self.down:\n",
        "            return self.down_forward(x)\n",
        "        else:\n",
        "            return self.up_forward(x, skip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGoQnKUSLs6-",
        "colab_type": "text"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f_hg-DDNOkp",
        "colab_type": "text"
      },
      "source": [
        "Generator class represents generator in the implementation. There are 8 UNetModules for down segments and 8 UNetModules for up segments. **forward()** simply calls forward() of each UNetModule (convolution-down and deconvolution-up segments).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76kAa_xWLvuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, num_filter, dropout):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.conv1 = UNetModule(input_size=in_channel, output_size=num_filter, batch_norm=False, activation=False,\n",
        "                                name=\"Conv1\")\n",
        "        self.conv2 = UNetModule(input_size=num_filter, output_size=num_filter * 2, name=\"Conv2\")\n",
        "        self.conv3 = UNetModule(input_size=num_filter * 2, output_size=num_filter * 4, name=\"Conv3\")\n",
        "        self.conv4 = UNetModule(input_size=num_filter * 4, output_size=num_filter * 8, dropout=dropout, name=\"Conv4\")\n",
        "        self.conv5 = UNetModule(input_size=num_filter * 8, output_size=num_filter * 8, dropout=dropout, name=\"Conv5\")\n",
        "        self.conv6 = UNetModule(input_size=num_filter * 8, output_size=num_filter * 8, dropout=dropout, name=\"Conv6\")\n",
        "        self.conv7 = UNetModule(input_size=num_filter * 8, output_size=num_filter * 8, dropout=dropout, name=\"Conv7\")\n",
        "        self.conv8 = UNetModule(input_size=num_filter * 8, output_size=num_filter * 8, dropout=dropout, name=\"Conv8\")\n",
        "\n",
        "        # decoder\n",
        "        self.deconv1 = UNetModule(down=False, input_size=num_filter * 8, output_size=num_filter * 8, dropout=dropout,\n",
        "                                  name=\"Deconv1\")\n",
        "        self.deconv2 = UNetModule(down=False, input_size=num_filter * 16, output_size=num_filter * 8, dropout=dropout,\n",
        "                                  name=\"Deconv2\")\n",
        "        self.deconv3 = UNetModule(down=False, input_size=num_filter * 16, output_size=num_filter * 8, dropout=dropout,\n",
        "                                  name=\"Deconv3\")\n",
        "        self.deconv4 = UNetModule(down=False, input_size=num_filter * 16, output_size=num_filter * 8, name=\"Deconv4\")\n",
        "        self.deconv5 = UNetModule(down=False, input_size=num_filter * 16, output_size=num_filter * 4, name=\"Deconv5\")\n",
        "        self.deconv6 = UNetModule(down=False, input_size=num_filter * 8, output_size=num_filter * 2, name=\"Deconv6\")\n",
        "        self.deconv7 = UNetModule(down=False, input_size=num_filter * 4, output_size=num_filter, name=\"Deconv7\")\n",
        "        self.deconv8 = UNetModule(down=False, input_size=num_filter * 2, output_size=out_channel, batch_norm=False,\n",
        "                                  output_layer=True, name=\"Deconv8\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.conv1(x)\n",
        "        d2 = self.conv2(d1)\n",
        "        d3 = self.conv3(d2)\n",
        "        d4 = self.conv4(d3)\n",
        "        d5 = self.conv5(d4)\n",
        "        d6 = self.conv6(d5)\n",
        "        d7 = self.conv7(d6)\n",
        "        d8 = self.conv8(d7)\n",
        "\n",
        "        u1 = self.deconv1(d8, d7)\n",
        "        u2 = self.deconv2(u1, d6)\n",
        "        u3 = self.deconv3(u2, d5)\n",
        "        u4 = self.deconv4(u3, d4)\n",
        "        u5 = self.deconv5(u4, d3)\n",
        "        u6 = self.deconv6(u5, d2)\n",
        "        u7 = self.deconv7(u6, d1)\n",
        "        out = self.deconv8(u7)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def initialize(self):\n",
        "        self.apply(initializer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5gvjfrYLyFt",
        "colab_type": "text"
      },
      "source": [
        "### DiscModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97dWN2e-OKQN",
        "colab_type": "text"
      },
      "source": [
        "This class is used to represent partitions of the discriminator. \n",
        "\n",
        "Following operations are done while forwarding data to next DiscModules:\n",
        "\n",
        "1.   Convolutional block is applied.\n",
        "2.   Batch normalization block is applied.\n",
        "3.   Activation block is applied.\n",
        "\n",
        "Not every layer of discriminator has the exact same configurations, some of the operations mentioned above may not be applied to a particular DiscModule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiezzBWmL0O5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscModule(nn.Module):\n",
        "    def __init__(self, input_size, output_size, batch_norm=True, stride=2):\n",
        "        super(DiscModule, self).__init__()\n",
        "\n",
        "        self.conv_block = nn.Conv2d(in_channels=input_size, out_channels=output_size,\n",
        "                                    kernel_size=4, stride=stride, padding=1)\n",
        "        self.act_block = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.bn_block = nn.BatchNorm2d(output_size, 0.8)\n",
        "\n",
        "        if not batch_norm:\n",
        "            self.bn_block = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.bn_block:\n",
        "            out = self.act_block(self.bn_block(self.conv_block(x)))\n",
        "        else:\n",
        "            out = self.act_block(self.conv_block(x))\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfiAPDQTL2Lb",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjUek-WRduP",
        "colab_type": "text"
      },
      "source": [
        "Discriminator class represents discriminator in the implementation. There are 5 DiscModules for 5 layers. **forward()** simply calls forward() of each DiscModule.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnvVvn9L4Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, num_filter):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = DiscModule(input_size=in_channel, output_size=num_filter, batch_norm=False)\n",
        "        self.conv2 = DiscModule(input_size=num_filter, output_size=num_filter * 2)\n",
        "        self.conv3 = DiscModule(input_size=num_filter * 2, output_size=num_filter * 4)\n",
        "        self.conv4 = DiscModule(num_filter * 4, num_filter * 8, stride=1)\n",
        "        self.conv5 = DiscModule(num_filter * 8, out_channel, stride=1, batch_norm=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def initialize(self):\n",
        "        self.apply(initializer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccgEhzKF6gg",
        "colab_type": "text"
      },
      "source": [
        "# Image Loader Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqs0iNt9F3uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageLoader(DataLoader):\n",
        "    def __init__(self, root, batch_size, image_size, mode=\"train\"):\n",
        "        super(ImageLoader, self).__init__(ImageDataset(root, image_size=image_size, mode=mode), batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=1)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, image_size, mode):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size), Image.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "        # Get images from two sides\n",
        "        self.filesA = sorted(glob.glob(os.path.join(root, mode) + \"/A/*.*\"))\n",
        "        self.filesB = sorted(glob.glob(os.path.join(root, mode) + \"/B/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            self.transform(Image.open(self.filesA[index % len(self.filesA)])),\n",
        "            self.transform(Image.open(self.filesB[index % len(self.filesB)]))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filesA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVTCRwuX1lw",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDvHwNPnX3Qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncated_normal_(tensor_, mean=0.0, std=1.0):\n",
        "    size = tensor_.shape\n",
        "    tmp = tensor_.new_empty(size + (4,)).normal_()\n",
        "    valid = (tmp < 2) & (tmp > -2)\n",
        "    ind = valid.max(-1, keepdim=True)[1]\n",
        "    tensor_.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
        "    tensor_.data.mul_(std).add_(mean)\n",
        "    return tensor_\n",
        "\n",
        "\n",
        "def initializer(m):\n",
        "    if isinstance(m, modules.conv.Conv2d):\n",
        "        # Weight\n",
        "        truncated_normal_(m.weight, 0.0, 0.02)\n",
        "        # Bias\n",
        "        init.constant_(m.bias, 0.0)\n",
        "\n",
        "    elif isinstance(m, modules.conv.ConvTranspose2d):\n",
        "        # Weight\n",
        "        init.normal_(m.weight, 0.0, 0.02)\n",
        "        # Bias\n",
        "        init.constant_(m.bias, 0.0)\n",
        "\n",
        "    elif isinstance(m, modules.batchnorm.BatchNorm2d):\n",
        "        # Scale\n",
        "        init.normal_(m.weight, 1.0, 0.02)\n",
        "        # Center\n",
        "        init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "def apply_gradient(float_tensor, interpolated, interpolates):\n",
        "    gradients = autograd.grad(\n",
        "        outputs=interpolated,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=Variable(float_tensor(np.ones(interpolated.shape)), requires_grad=False),\n",
        "        create_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UDjagjp9F-Y",
        "colab_type": "text"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_ntJEzH95v5",
        "colab_type": "text"
      },
      "source": [
        "This function is used for generating samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9uj2wNY97Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take_sample(loader, float_tensor, G_AtoB, G_BtoA, dataset_name, batch_idx):\n",
        "    print(\"Generating samples\")\n",
        "    images = next(iter(loader))\n",
        "    imagesA = Variable(images[0].type(float_tensor))\n",
        "    imagesB = Variable(images[1].type(float_tensor))\n",
        "\n",
        "    fake_b = G_AtoB(imagesA)\n",
        "    fake_a = G_BtoA(imagesB)\n",
        "    sample = torch.cat((imagesA, imagesB, fake_b, fake_a))\n",
        "    save_image(sample, \"training_outputs/%s_%s.png\" % (dataset_name, batch_idx), nrow=8, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zBfxMy7RWo3",
        "colab_type": "text"
      },
      "source": [
        "This function is run when the test mode is active:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5USzka4JNEFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_mode(dataset_name, G_AtoB, G_BtoA, val_loader):\n",
        "    # Download and load pre-trained models if not exists\n",
        "    # facades GAtoB: https://drive.google.com/open?id=1pbO2jL1-X_1_OTWV0ZwcVavMA0i4vfiz\n",
        "    # facades GBtoA: https://drive.google.com/open?id=1QjIg6RIbSmEmgF7Lzvu1XgfM9_JQRA5X\n",
        "\n",
        "    # maps GAtoB: https://drive.google.com/file/d/1njSobeoyU576WbgnieQmnCzHpOYvVkf_\n",
        "    # maps GBtoA: https://drive.google.com/file/d/1oQUaFHZf9dGHvRuLvRl_qtt_ihyJDTCi\n",
        "    gatob_pretrained_filename = \"%s_GAtoB.pth\" % dataset_name\n",
        "    gbtoa_pretrained_filename = \"%s_GBtoA.pth\" % dataset_name\n",
        "\n",
        "    if gatob_pretrained_filename in os.listdir(\"./training_outputs\") and gbtoa_pretrained_filename in os.listdir(\"./training_outputs\"):\n",
        "        print(\"Found pre-trained models, skipping downloading\")\n",
        "    else:\n",
        "        print(\"Downloading pre-trained models\")\n",
        "\n",
        "        if dataset_name == \"maps\":\n",
        "            gdd.download_file_from_google_drive(file_id='1njSobeoyU576WbgnieQmnCzHpOYvVkf_', dest_path='./training_outputs/' + gatob_pretrained_filename, unzip=True, overwrite=True)\n",
        "            gdd.download_file_from_google_drive(file_id='1oQUaFHZf9dGHvRuLvRl_qtt_ihyJDTCi', dest_path='./training_outputs/' + gbtoa_pretrained_filename, unzip=True, overwrite=True)\n",
        "\n",
        "        elif dataset_name == \"facades\":\n",
        "            gdd.download_file_from_google_drive(file_id='1pbO2jL1-X_1_OTWV0ZwcVavMA0i4vfiz', dest_path='./training_outputs/' + gatob_pretrained_filename, unzip=True, overwrite=True)\n",
        "            gdd.download_file_from_google_drive(file_id='1QjIg6RIbSmEmgF7Lzvu1XgfM9_JQRA5X', dest_path='./training_outputs/' + gbtoa_pretrained_filename, unzip=True, overwrite=True)\n",
        "\n",
        "        else:\n",
        "            print(\"This dataset is not supported.\")\n",
        "            return\n",
        "\n",
        "    print(\"Loading pre-trained models\")\n",
        "    G_AtoB.load_state_dict(torch.load(\"training_outputs/%s_GAtoB.pth\" % dataset_name, map_location=\"cpu\"))\n",
        "    G_BtoA.load_state_dict(torch.load(\"training_outputs/%s_GBtoA.pth\" % dataset_name, map_location=\"cpu\"))\n",
        "    take_sample(val_loader, FloatTensor, G_AtoB, G_BtoA, dataset_name, batch_idx=\"all\")\n",
        "\n",
        "    print(\"See results in training_outputs/%s_all.png\" % dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNja8TONRfEj",
        "colab_type": "text"
      },
      "source": [
        "This function is run for initializing optimizers for training mode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVIBlu7lNvRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_optimizers(G_AtoB, G_BtoA, learning_rate, betas):\n",
        "    print(\"Initializing optimizers\")\n",
        "    g_optim = torch.optim.Adam(\n",
        "        itertools.chain(G_AtoB.parameters(), G_BtoA.parameters()), lr=learning_rate, betas=betas\n",
        "    )\n",
        "    da_optim = torch.optim.Adam(D_A.parameters(), lr=learning_rate, betas=betas)\n",
        "    db_optim = torch.optim.Adam(D_B.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    return g_optim, da_optim, db_optim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQh13qrG7Xfj",
        "colab_type": "text"
      },
      "source": [
        "This function is run for downloading the dataset if the file does not exist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocL48v7F7UiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(folder_location, dataset_name=\"facades\"):\n",
        "    dataset_filename = \"%s.zip\" % dataset_name\n",
        "    destination = \"%s/%s\" % (folder_location, dataset_filename)\n",
        "\n",
        "    if dataset_filename in os.listdir(folder_location):\n",
        "        print(\"The dataset file already exists in %s, skipping downloading\" % destination)\n",
        "    else:\n",
        "        print(\"Downloading the dataset: %s\" % dataset_name)\n",
        "\n",
        "        if dataset_name == \"maps\":\n",
        "            google_drive_file_id = \"1J-xVnCOcq986gZrvG3INmqtP8htbyOPG\"\n",
        "\n",
        "        else:\n",
        "            google_drive_file_id = \"1__3713Jm0YoXMReU43qwkgz7n6BwOmAW\"\n",
        "\n",
        "        # facades.zip link: https://drive.google.com/open?id=1__3713Jm0YoXMReU43qwkgz7n6BwOmAW\n",
        "        # maps.zip link: https://drive.google.com/file/d/1J-xVnCOcq986gZrvG3INmqtP8htbyOPG\n",
        "        gdd.download_file_from_google_drive(file_id=google_drive_file_id, dest_path=destination, unzip=True, overwrite=True)\n",
        "\n",
        "    print(\"Dataset '%s' is ready to use.\" % dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vhPTv5vSqXj",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-Parameters and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqck9bc-UEO2",
        "colab_type": "text"
      },
      "source": [
        "### Configuration\n",
        "\n",
        "By default, this notebook is configured to run on the **test mode**. Test mode allows you to load pre-trained models and get the results according to their implicit values. To run in **training mode**, you should change \"test\" parameter of \"opt\" to **False**.\n",
        "\n",
        "In test mode, pre-trained models are downloaded from a Google Drive directory. Once the pre-trained models are downloaded, they are ready to use. Downloading them once is enough.\n",
        "\n",
        "By default:\n",
        "\n",
        "*   5 * 10^-5 is used as learning rate\n",
        "*   L1-loss is used as loss function. If \"loss_metric\" is not specified, mean squared error loss function is used by default.\n",
        "*   50 epochs are run in training mode\n",
        "*   Dataset name is **\"facades\"** by default. Other dataset alternative is **\"maps\"**. When \"maps\" dataset is loaded, **batch_size = 3** is recommended.\n",
        "*   Default running mode is \"test mode\"\n",
        "*   Default number of samples generated at the end is 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-T0rDn9SuWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = {\n",
        "    \"loss_metric\": \"L1\",\n",
        "    \"img_size\": 256,\n",
        "    \"filter_size\": 64,\n",
        "    \"channels\": 3,\n",
        "    \"dropout_rate\": 0.5,\n",
        "    \"lr\":  0.00005,\n",
        "    \"beta1\": 0.5,\n",
        "    \"beta2\": 0.900,\n",
        "    \"num_epochs\": 50,\n",
        "    \"batch_size\": 4,\n",
        "    \"lambda_A\": 500.0,\n",
        "    \"lambda_B\": 500.0,\n",
        "    \"dataset_name\": \"facades\",\n",
        "    \"n_cpu\": 8,\n",
        "    \"n_critic\": 1,\n",
        "    \"test\": True,\n",
        "    \"sample_count\": 4,\n",
        "    \"save_period\": 1,\n",
        "    \"log_freq\": 10\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5XgTBrG-WMK",
        "colab_type": "text"
      },
      "source": [
        "Dataset is downloaded and used from the directory called **\"training_inputs\"**. Pre-trained models are downloaded and samples are generated to the directory called **\"training_outputs\"**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoYIle3f4e4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1dd9177-12c3-4ed1-d4f7-538f3cdb11f8"
      },
      "source": [
        "# make folders for inputs and outputs\n",
        "os.makedirs(\"training_outputs\", exist_ok=True)\n",
        "os.makedirs(\"training_inputs\", exist_ok=True)\n",
        "\n",
        "# configuration params\n",
        "img_shape = (opt[\"channels\"], opt[\"img_size\"], opt[\"img_size\"])\n",
        "gamma = 10.\n",
        "\n",
        "if opt[\"loss_metric\"] == 'L1':\n",
        "    norm_loss = torch.nn.L1Loss()\n",
        "else:\n",
        "    norm_loss = torch.nn.MSELoss()\n",
        "  \n",
        "# download the dataset if it does not exist in 'training_inputs' folder\n",
        "prepare_dataset(folder_location=\"./training_inputs\", dataset_name=opt[\"dataset_name\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset file already exists in ./training_inputs/maps.zip, skipping downloading\n",
            "Dataset 'maps' is ready to use.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeaCKCd_WrFH",
        "colab_type": "text"
      },
      "source": [
        "### Training or Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4cVtwplUMi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6c0b4aba-be7c-43e9-cea5-2166669679e6"
      },
      "source": [
        "# create models\n",
        "G_AtoB = Generator(opt[\"channels\"], opt[\"channels\"], opt[\"filter_size\"], opt[\"dropout_rate\"])\n",
        "G_BtoA = Generator(opt[\"channels\"], opt[\"channels\"], opt[\"filter_size\"], opt[\"dropout_rate\"])\n",
        "D_A = Discriminator(opt[\"channels\"], 1, opt[\"filter_size\"])\n",
        "D_B = Discriminator(opt[\"channels\"], 1, opt[\"filter_size\"])\n",
        "\n",
        "if cuda:\n",
        "    G_AtoB.cuda()\n",
        "    G_BtoA.cuda()\n",
        "    D_A.cuda()\n",
        "    D_B.cuda()\n",
        "    norm_loss.cuda()\n",
        "\n",
        "# create ImageLoader classes for training and validation\n",
        "data_loader = ImageLoader(root=\"./training_inputs/%s\" % opt[\"dataset_name\"], batch_size=opt[\"batch_size\"], image_size=opt[\"img_size\"])\n",
        "val_loader = ImageLoader(root=\"./training_inputs/%s\" % opt[\"dataset_name\"], batch_size=opt[\"sample_count\"], image_size=opt[\"img_size\"], mode=\"val\")\n",
        "\n",
        "# if test mode is selected then generate samples\n",
        "if opt[\"test\"]:\n",
        "    print(\"Running mode: TEST\")\n",
        "    test_mode(opt[\"dataset_name\"], G_AtoB, G_BtoA, val_loader)\n",
        "\n",
        "else:\n",
        "    print(\"Running mode: TRAINING\")\n",
        "    g_optim, da_optim, db_optim = init_optimizers(G_AtoB, G_BtoA, opt[\"lr\"], (opt[\"beta1\"], opt[\"beta2\"]))\n",
        "\n",
        "    total_batch = 0\n",
        "\n",
        "    for epoch in range(opt[\"num_epochs\"]):\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "            da_optim.zero_grad()\n",
        "            db_optim.zero_grad()\n",
        "\n",
        "            # Configure input\n",
        "            imagesA = Variable(batch[0].type(FloatTensor))\n",
        "            imagesB = Variable(batch[1].type(FloatTensor))\n",
        "\n",
        "            # Train discriminators in each epoch\n",
        "\n",
        "            # Generate a batch of images\n",
        "            fake_A = G_BtoA(imagesB).detach()\n",
        "            fake_B = G_AtoB(imagesA).detach()\n",
        "            fake_ABA = G_BtoA(fake_B)\n",
        "            fake_BAB = G_AtoB(fake_A)\n",
        "\n",
        "            # wgan-gp training\n",
        "            epsilon_A = FloatTensor(np.random.random((opt[\"batch_size\"], 1, 1, 1)))\n",
        "            epsilon_B = FloatTensor(np.random.random((opt[\"batch_size\"], 1, 1, 1)))\n",
        "\n",
        "            interpolated_image_A = (imagesA.data + epsilon_A * (fake_A.data - imagesA.data)).requires_grad_(True)\n",
        "            interpolated_image_B = (imagesB.data + epsilon_B * (fake_B.data - imagesB.data)).requires_grad_(True)\n",
        "            d_interpolated_A = D_A(interpolated_image_A)\n",
        "            d_interpolated_B = D_B(interpolated_image_B)\n",
        "            gradA = apply_gradient(FloatTensor, d_interpolated_A, interpolated_image_A)\n",
        "            gradB = apply_gradient(FloatTensor, d_interpolated_B, interpolated_image_B)\n",
        "\n",
        "            # - real image score + fake image score + gp\n",
        "            disc_loss = (torch.mean(D_A(fake_A)) + torch.mean(D_B(fake_B))) - (\n",
        "                        torch.mean(D_A(imagesA)) + torch.mean(D_B(imagesB))) + gamma * (gradA + gradB)\n",
        "\n",
        "            disc_loss.backward()\n",
        "            da_optim.step()\n",
        "            db_optim.step()\n",
        "\n",
        "            if batch_idx % opt[\"n_critic\"] == 0:\n",
        "                # train generator after each n_critic batch loop\n",
        "                g_optim.zero_grad()\n",
        "\n",
        "                ga_loss = -torch.mean(D_A(fake_A)) + opt[\"lambda_A\"] * norm_loss(fake_ABA, imagesA)\n",
        "                gb_loss = -torch.mean(D_B(fake_B)) + opt[\"lambda_B\"] * norm_loss(fake_BAB, imagesB)\n",
        "                gen_loss = ga_loss + gb_loss\n",
        "\n",
        "                gen_loss.backward()\n",
        "                g_optim.step()\n",
        "            if batch_idx % opt[\"log_freq\"] == 0:\n",
        "                print(\"Iteration: %d - Batch: %d - D-Loss: %.5f - G-Loss: %.5f\" % (epoch, batch_idx, disc_loss, gen_loss))\n",
        "                take_sample(val_loader, FloatTensor, G_AtoB, G_BtoA, opt[\"dataset_name\"], total_batch)\n",
        "\n",
        "            total_batch += 1\n",
        "\n",
        "        if epoch % opt[\"save_period\"] == 0:\n",
        "            torch.save(G_AtoB.state_dict(), \"training_outputs/%s_GAtoB.pth\" % opt[\"dataset_name\"])\n",
        "            torch.save(G_BtoA.state_dict(), \"training_outputs/%s_GBtoA.pth\" % opt[\"dataset_name\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running mode: TEST\n",
            "Downloading pre-trained models\n",
            "Downloading 1njSobeoyU576WbgnieQmnCzHpOYvVkf_ into ./training_outputs/maps_GAtoB.pth... Done.\n",
            "Unzipping...Downloading 1oQUaFHZf9dGHvRuLvRl_qtt_ihyJDTCi into ./training_outputs/maps_GBtoA.pth... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google_drive_downloader/google_drive_downloader.py:78: UserWarning: Ignoring `unzip` since \"1njSobeoyU576WbgnieQmnCzHpOYvVkf_\" does not look like a valid zip file\n",
            "  warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Unzipping...Loading pre-trained models\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google_drive_downloader/google_drive_downloader.py:78: UserWarning: Ignoring `unzip` since \"1oQUaFHZf9dGHvRuLvRl_qtt_ihyJDTCi\" does not look like a valid zip file\n",
            "  warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating samples\n",
            "See results in training_outputs/maps_all.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h1u3TUO1kRS",
        "colab_type": "text"
      },
      "source": [
        "# Result of Pre-trained Generator Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx7HypdU2oWG",
        "colab_type": "text"
      },
      "source": [
        "# Our Results:\n",
        "\n",
        "Input dataset: \"facades\"\n",
        "\n",
        "![result_facades](https://i.hizliresim.com/1xpJVS.png)\n",
        "\n",
        "\n",
        "\n",
        "Input dataset: \"maps\"\n",
        "\n",
        "![result_maps](https://i.hizliresim.com/JEOspo.png)\n",
        "\n",
        "Evolution of the output (facades):\n",
        "\n",
        "![evolution_maps](https://i.hizliresim.com/d9U0vL.gif)\n",
        "\n",
        "Evolution of the output (maps):\n",
        "\n",
        "![evolution_maps](https://i.hizliresim.com/92Vkhs.gif)\n",
        "\n",
        "While creating the gifs above, ImageMagick's \"convert\" command is used to convert outputs of each batch in each epoch:\n",
        "\n",
        "```\n",
        ">_ convert -delay 200 -loop 0 maps*.png out.gif\n",
        "```\n",
        "\n",
        "# Figure 3 and Figure 8 from the paper:\n",
        "![figure_3](https://i.hizliresim.com/KCCTIn.png)\n",
        "\n",
        "![figure_8](https://i.hizliresim.com/4zt3H3.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQktbaQi8-YT",
        "colab_type": "text"
      },
      "source": [
        "# Difficulties Encountered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KathIHs9Cnl",
        "colab_type": "text"
      },
      "source": [
        "*    U-Net shaped network is used in the paper. It's details do not explained. We analyzed the referenced paper first to understand U-net shaped (number of layers, skips etc.). Since it is not complex, it was not take too much time to implement.\n",
        "*    WGAN-GP is used in official tensorflow implementation as default and it has better performance. However, paper does not contain this approach.\n",
        "*    Paper briefly mentions the usage and not much details are shared with readers."
      ]
    }
  ]
}